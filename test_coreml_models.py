#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä—É–µ–º CoreML –º–æ–¥–µ–ª–∏ –Ω–∞–ø—Ä—è–º—É—é
"""
import coremltools as ct
import numpy as np
import cv2

def test_models():
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ CoreML –º–æ–¥–µ–ª–µ–π...")
    
    # FastDVDnet
    try:
        print("\n1Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É—é FastDVDnet...")
        fastdvd_model = ct.models.MLModel("FastDVDnet.mlpackage")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –≤—Ö–æ–¥ [1, 15, 256, 256]
        test_input = np.random.rand(1, 15, 256, 256).astype(np.float32)
        input_dict = {"noisy": test_input}
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        output = fastdvd_model.predict(input_dict)
        output_array = output["denoised"]
        
        print(f"‚úÖ FastDVDnet: {test_input.shape} -> {output_array.shape}")
        
    except Exception as e:
        print(f"‚ùå FastDVDnet –æ—à–∏–±–∫–∞: {e}")
    
    # RealBasicVSR
    try:
        print("\n2Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É—é RealBasicVSR...")
        rbv_model = ct.models.MLModel("RealBasicVSR_x2.mlpackage")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –≤—Ö–æ–¥ [1, 3, 256, 256]
        test_input = np.random.rand(1, 3, 256, 256).astype(np.float32)
        input_dict = {"input": test_input}
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        output = rbv_model.predict(input_dict)
        output_array = output["output"]
        
        print(f"‚úÖ RealBasicVSR: {test_input.shape} -> {output_array.shape}")
        
    except Exception as e:
        print(f"‚ùå RealBasicVSR –æ—à–∏–±–∫–∞: {e}")

def test_video_frame():
    """–¢–µ—Å—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º –∫–∞–¥—Ä–æ–º"""
    try:
        print("\nüé¨ –¢–µ—Å—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º –≤–∏–¥–µ–æ –∫–∞–¥—Ä–æ–º...")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–¥–∏–Ω –∫–∞–¥—Ä –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ
        cap = cv2.VideoCapture("test_video.mp4")
        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–¥—Ä")
            return
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–∞–¥—Ä –¥–ª—è RealBasicVSR
        h, w = frame.shape[:2]
        print(f"–ò—Å—Ö–æ–¥–Ω—ã–π –∫–∞–¥—Ä: {w}x{h}")
        
        # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ 256x256 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª—å—é
        frame_resized = cv2.resize(frame, (256, 256))
        
        # BGR -> RGB –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
        
        # HWC -> NCHW
        frame_tensor = np.transpose(frame_rgb, (2, 0, 1))[np.newaxis, ...]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º RealBasicVSR
        rbv_model = ct.models.MLModel("RealBasicVSR_x2.mlpackage")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        input_dict = {"input": frame_tensor}
        output = rbv_model.predict(input_dict)
        output_array = output["output"]
        
        print(f"‚úÖ –†–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç: {frame_tensor.shape} -> {output_array.shape}")
        
        # NCHW -> HWC
        output_image = np.transpose(output_array[0], (1, 2, 0))
        output_image = np.clip(output_image * 255, 0, 255).astype(np.uint8)
        
        # RGB -> BGR
        output_bgr = cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        cv2.imwrite("test_upscaled_frame.png", output_bgr)
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç: test_upscaled_frame.png ({output_bgr.shape[1]}x{output_bgr.shape[0]})")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞ —Å –∫–∞–¥—Ä–æ–º: {e}")

if __name__ == "__main__":
    test_models()
    test_video_frame()